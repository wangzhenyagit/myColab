{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wangzhenyagit/myColab/blob/main/autogen_agentchat_groupchat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "o_fFgnpByyJB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3644cbf8-392a-485b-b3d3-dc2b5c1e21dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyautogen>=0.2.3 in /usr/local/lib/python3.10/dist-packages (0.2.8)\n",
            "Requirement already satisfied: diskcache in /usr/local/lib/python3.10/dist-packages (from pyautogen>=0.2.3) (5.6.3)\n",
            "Requirement already satisfied: docker in /usr/local/lib/python3.10/dist-packages (from pyautogen>=0.2.3) (7.0.0)\n",
            "Requirement already satisfied: flaml in /usr/local/lib/python3.10/dist-packages (from pyautogen>=0.2.3) (2.1.1)\n",
            "Requirement already satisfied: openai>=1.3 in /usr/local/lib/python3.10/dist-packages (from pyautogen>=0.2.3) (1.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.10 in /usr/local/lib/python3.10/dist-packages (from pyautogen>=0.2.3) (1.10.13)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from pyautogen>=0.2.3) (1.0.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from pyautogen>=0.2.3) (2.4.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from pyautogen>=0.2.3) (0.5.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen>=0.2.3) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.3->pyautogen>=0.2.3) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen>=0.2.3) (0.26.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen>=0.2.3) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen>=0.2.3) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->pyautogen>=0.2.3) (4.9.0)\n",
            "Requirement already satisfied: packaging>=14.0 in /usr/local/lib/python3.10/dist-packages (from docker->pyautogen>=0.2.3) (23.2)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->pyautogen>=0.2.3) (2.31.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->pyautogen>=0.2.3) (2.0.7)\n",
            "Requirement already satisfied: NumPy>=1.17.0rc1 in /usr/local/lib/python3.10/dist-packages (from flaml->pyautogen>=0.2.3) (1.23.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->pyautogen>=0.2.3) (2023.6.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3->pyautogen>=0.2.3) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3->pyautogen>=0.2.3) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.3->pyautogen>=0.2.3) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.3->pyautogen>=0.2.3) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3->pyautogen>=0.2.3) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->docker->pyautogen>=0.2.3) (3.3.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"pyautogen>=0.2.3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "664aVtgayyJB"
      },
      "source": [
        "## Set your API Endpoint\n",
        "\n",
        "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxLwMP3Q0ILu",
        "outputId": "9a126613-6f38-4716-9319-1103f9e7f77e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xZomEcBByyJB"
      },
      "outputs": [],
      "source": [
        "import autogen\n",
        "\n",
        "config_list_gpt4 = autogen.config_list_from_json(\n",
        "    \"OAI_CONFIG_LIST\",\n",
        "    filter_dict={\n",
        "        \"model\": [\"gpt-4\"],\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OApLijstyyJC"
      },
      "outputs": [],
      "source": [
        "llm_config = {\"config_list\": config_list_gpt4, \"cache_seed\": 42}\n",
        "user_proxy = autogen.UserProxyAgent(\n",
        "    name=\"User_proxy\",\n",
        "    system_message=\"A human admin.\",\n",
        "    code_execution_config={\n",
        "        \"last_n_messages\": 2,\n",
        "        \"work_dir\": \"groupchat\",\n",
        "        \"use_docker\": False,\n",
        "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
        "    human_input_mode=\"TERMINATE\",\n",
        ")\n",
        "coder = autogen.AssistantAgent(\n",
        "    name=\"Coder\",\n",
        "    llm_config=llm_config,\n",
        ")\n",
        "pm = autogen.AssistantAgent(\n",
        "    name=\"Product_manager\",\n",
        "    system_message=\"Creative in software product ideas.\",\n",
        "    llm_config=llm_config,\n",
        ")\n",
        "groupchat = autogen.GroupChat(agents=[user_proxy, coder, pm], messages=[], max_round=12)\n",
        "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0OBIO_GyyJC"
      },
      "source": [
        "## Start Chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "36XOlUY6yyJC",
        "outputId": "1824310e-d2d5-454c-de75-1a87332a8ab9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User_proxy (to chat_manager):\n",
            "\n",
            "Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Coder (to chat_manager):\n",
            "\n",
            "To complete this task, I will use the ArXiv API to search for the latest papers about GPT-4. The ArXiv API allows us to query their database and return results. I will then parse these results to find any potential applications in software.\n",
            "\n",
            "First, it's important to install the required library to query the ArXiv API. You can do this by running the following command:\n",
            "\n",
            "```python\n",
            "pip install arxiv\n",
            "```\n",
            "\n",
            "Then, we can execute the Python script for fetching the latest papers about GPT-4.\n",
            "\n",
            "```python\n",
            "# filename: arxiv_gpt4_search.py\n",
            "import arxiv\n",
            "\n",
            "try:\n",
            "    search = arxiv.Search(\n",
            "        query=\"gpt-4\",\n",
            "        max_results=10,\n",
            "        sort_by=arxiv.SortCriterion.SubmittedDate\n",
            "    )\n",
            "\n",
            "    for result in search.get():\n",
            "        print(\"Paper title: \", result.title)\n",
            "        print(\"Summary: \", result.summary)\n",
            "        print(\"Published on: \", result.published)\n",
            "        print(\"URL: \", result.pdf_url)\n",
            "        print(\"------\")\n",
            "except Exception as e:\n",
            "    print(f\"An error occurred: {str(e)}\")\n",
            "```\n",
            "\n",
            "This script will query the ArXiv API to search for the most recent papers related to GPT-4. It prints out the paper's title, summary, published date, and PDF URL. You will need to run this script and check the summary of each paper to find its potential applications in software. If a paper seems relevant, you can visit the provided URL to read the full paper.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "\n",
            ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\n",
            "User_proxy (to chat_manager):\n",
            "\n",
            "exitcode: 1 (execution failed)\n",
            "Code output: \n",
            "  File \"\", line 1\n",
            "    pip install arxiv\n",
            "        ^^^^^^^\n",
            "SyntaxError: invalid syntax\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Coder (to chat_manager):\n",
            "\n",
            "My apologies for the confusion. The correct way to install python packages should be done outside of the Python environment, in the shell itself. Please run the below shell command to install the required python package.\n",
            "```sh\n",
            "pip install arxiv\n",
            "```\n",
            "After installing the package, please run the python code shared above in a python environment.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "\n",
            ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is sh)...\n",
            "User_proxy (to chat_manager):\n",
            "\n",
            "exitcode: 0 (execution succeeded)\n",
            "Code output: \n",
            "Collecting arxiv\n",
            "  Downloading arxiv-2.1.0-py3-none-any.whl (11 kB)\n",
            "Collecting feedparser==6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.1/81.1 kB 1.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from arxiv) (2.31.0)\n",
            "Collecting sgmllib3k (from feedparser==6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv) (2023.11.17)\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py): started\n",
            "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6048 sha256=28d8cc894335a1532a11849d0be73d58fa6f12b7561c2034c835bbe853083bc5\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-2.1.0 feedparser-6.0.10 sgmllib3k-1.0.0\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "\n",
            ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is sh)...\n",
            "User_proxy (to chat_manager):\n",
            "\n",
            "exitcode: 0 (execution succeeded)\n",
            "Code output: \n",
            "Requirement already satisfied: arxiv in /usr/local/lib/python3.10/dist-packages (2.1.0)\n",
            "Requirement already satisfied: feedparser==6.0.10 in /usr/local/lib/python3.10/dist-packages (from arxiv) (6.0.10)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from arxiv) (2.31.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser==6.0.10->arxiv) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv) (2023.11.17)\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "User_proxy (to chat_manager):\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Coder (to chat_manager):\n",
            "\n",
            "It seems like the `arxiv` package is installed correctly now. Let's proceed with the Python script for fetching the latest papers about GPT-4.\n",
            "\n",
            "Here is the Python script:\n",
            "\n",
            "```python\n",
            "# filename: arxiv_gpt4_search.py\n",
            "import arxiv\n",
            "\n",
            "try:\n",
            "    search = arxiv.Search(\n",
            "        query=\"gpt-4\",\n",
            "        max_results=10,\n",
            "        sort_by=arxiv.SortCriterion.SubmittedDate\n",
            "    )\n",
            "\n",
            "    for result in search.get():\n",
            "        print(\"Paper title: \", result.title)\n",
            "        print(\"Summary: \", result.summary)\n",
            "        print(\"Published on: \", result.published)\n",
            "        print(\"URL: \", result.pdf_url)\n",
            "        print(\"------\")\n",
            "except Exception as e:\n",
            "    print(f\"An error occurred: {str(e)}\")\n",
            "```\n",
            "\n",
            "Please run this script in a Python environment. This will fetch the latest papers about GPT-4. Among the returned results, look for papers that discuss the potential applications of GPT-4 in software.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "\n",
            ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\n",
            "User_proxy (to chat_manager):\n",
            "\n",
            "exitcode: 0 (execution succeeded)\n",
            "Code output: \n",
            "An error occurred: 'Search' object has no attribute 'get'\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Coder (to chat_manager):\n",
            "\n",
            "I apologize for the oversight. The arxiv module's Search object doesn't have a 'get' method, I was incorrect in my previous instruction.\n",
            "\n",
            "The correct way to iterate over the Search object is directly in a for loop. Here's the corrected code:\n",
            "\n",
            "```python\n",
            "# filename: arxiv_gpt4_search.py\n",
            "import arxiv\n",
            "\n",
            "try:\n",
            "    search = arxiv.Search(\n",
            "        query=\"gpt-4\",\n",
            "        max_results=10,\n",
            "        sort_by=arxiv.SortCriterion.SubmittedDate\n",
            "    )\n",
            "\n",
            "    for result in search.results():\n",
            "        print(\"Paper title: \", result.title)\n",
            "        print(\"Summary: \", result.summary)\n",
            "        print(\"Published on: \", result.published)\n",
            "        print(\"URL: \", result.entry_id)\n",
            "        print(\"------\")\n",
            "except Exception as e:\n",
            "    print(f\"An error occurred: {str(e)}\")\n",
            "```\n",
            "Please execute this Python code to fetch the latest papers about GPT-4.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> USING AUTO REPLY...\n",
            "\n",
            ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\n",
            "User_proxy (to chat_manager):\n",
            "\n",
            "exitcode: 0 (execution succeeded)\n",
            "Code output: \n",
            "Paper title:  Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding\n",
            "Summary:  We introduce meta-prompting, an effective scaffolding technique designed to\n",
            "enhance the functionality of language models (LMs). This approach transforms a\n",
            "single LM into a multi-faceted conductor, adept at managing and integrating\n",
            "multiple independent LM queries. By employing high-level instructions,\n",
            "meta-prompting guides the LM to break down complex tasks into smaller, more\n",
            "manageable subtasks. These subtasks are then handled by distinct \"expert\"\n",
            "instances of the same LM, each operating under specific, tailored instructions.\n",
            "Central to this process is the LM itself, in its role as the conductor, which\n",
            "ensures seamless communication and effective integration of the outputs from\n",
            "these expert models. It additionally employs its inherent critical thinking and\n",
            "robust verification processes to refine and authenticate the end result. This\n",
            "collaborative prompting approach empowers a single LM to simultaneously act as\n",
            "a comprehensive orchestrator and a panel of diverse experts, significantly\n",
            "enhancing its performance across a wide array of tasks. The zero-shot,\n",
            "task-agnostic nature of meta-prompting greatly simplifies user interaction by\n",
            "obviating the need for detailed, task-specific instructions. Furthermore, our\n",
            "research demonstrates the seamless integration of external tools, such as a\n",
            "Python interpreter, into the meta-prompting framework, thereby broadening its\n",
            "applicability and utility. Through rigorous experimentation with GPT-4, we\n",
            "establish the superiority of meta-prompting over conventional scaffolding\n",
            "methods: When averaged across all tasks, including the Game of 24,\n",
            "Checkmate-in-One, and Python Programming Puzzles, meta-prompting, augmented\n",
            "with a Python interpreter functionality, surpasses standard prompting by 17.1%,\n",
            "expert (dynamic) prompting by 17.3%, and multipersona prompting by 15.2%.\n",
            "Published on:  2024-01-23 18:22:19+00:00\n",
            "URL:  http://arxiv.org/abs/2401.12954v1\n",
            "------\n",
            "Paper title:  KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning\n",
            "Summary:  Large Language Models (LLMs) have demonstrated impressive performance in\n",
            "natural language processing tasks by leveraging chain of thought (CoT) that\n",
            "enables step-by-step thinking. Extending LLMs with multimodal capabilities is\n",
            "the recent interest, but incurs computational cost and requires substantial\n",
            "hardware resources. To address these challenges, we propose KAM-CoT a framework\n",
            "that integrates CoT reasoning, Knowledge Graphs (KGs), and multiple modalities\n",
            "for a comprehensive understanding of multimodal tasks. KAM-CoT adopts a\n",
            "two-stage training process with KG grounding to generate effective rationales\n",
            "and answers. By incorporating external knowledge from KGs during reasoning, the\n",
            "model gains a deeper contextual understanding reducing hallucinations and\n",
            "enhancing the quality of answers. This knowledge-augmented CoT reasoning\n",
            "empowers the model to handle questions requiring external context, providing\n",
            "more informed answers. Experimental findings show KAM-CoT outperforms the\n",
            "state-of-the-art methods. On the ScienceQA dataset, we achieve an average\n",
            "accuracy of 93.87%, surpassing GPT-3.5 (75.17%) by 18% and GPT-4 (83.99%) by\n",
            "10%. Remarkably, KAM-CoT achieves these results with only 280M trainable\n",
            "parameters at a time, demonstrating its cost-efficiency and effectiveness.\n",
            "Published on:  2024-01-23 15:56:11+00:00\n",
            "URL:  http://arxiv.org/abs/2401.12863v1\n",
            "------\n",
            "Paper title:  SuperCLUE-Math6: Graded Multi-Step Math Reasoning Benchmark for LLMs in Chinese\n",
            "Summary:  We introduce SuperCLUE-Math6(SC-Math6), a new benchmark dataset to evaluate\n",
            "the mathematical reasoning abilities of Chinese language models. SC-Math6 is\n",
            "designed as an upgraded Chinese version of the GSM8K dataset with enhanced\n",
            "difficulty, diversity, and application scope. It consists of over 2000\n",
            "mathematical word problems requiring multi-step reasoning and providing natural\n",
            "language solutions. We propose an innovative scheme to quantify the reasoning\n",
            "capability of large models based on performance over problems with different\n",
            "reasoning steps. Experiments on 12 representative Chinese models demonstrate a\n",
            "clear stratification of reasoning levels, with top models like GPT-4 showing\n",
            "superior performance. SC-Math6 fills the gap in Chinese mathematical reasoning\n",
            "benchmarks and provides a comprehensive testbed to advance the intelligence of\n",
            "Chinese language models.\n",
            "Published on:  2024-01-22 10:30:11+00:00\n",
            "URL:  http://arxiv.org/abs/2401.11819v1\n",
            "------\n",
            "Paper title:  Speak It Out: Solving Symbol-Related Problems with Symbol-to-Language Conversion for Language Models\n",
            "Summary:  Symbols (or more broadly, non-natural language textual representations) such\n",
            "as numerical sequences, molecular formulas, and table delimiters widely exist,\n",
            "playing important roles in various tasks such as abstract reasoning, chemical\n",
            "property prediction, and table question answering. Despite the impressive\n",
            "natural language comprehension capabilities of large language models (LLMs),\n",
            "their reasoning abilities for symbols remain inadequate, which could attributed\n",
            "to the difference between symbol representations and general natural languages.\n",
            "We propose symbol-to-language (S2L), a tuning-free method that enables large\n",
            "language models to solve symbol-related problems with information expressed in\n",
            "natural language. Specifically, S2L first converts the symbols involved to\n",
            "language-based representations, which can be implemented by prompting LLMs or\n",
            "leveraging external tools, then these language-based representations are\n",
            "integrated into the original problem via direct substitution or concatenation,\n",
            "serving as useful input information for LLMs. We evaluate the S2L method using\n",
            "both API-based (GPT-4, ChatGPT) and open-source (OpenChat) models over eight\n",
            "symbol-related tasks, ranging from symbol-only abstract reasoning to sentiment\n",
            "analysis in social media. Experimental results show that S2L consistently leads\n",
            "to superior performance. For example, by employing S2L for GPT-4, there can be\n",
            "average significant improvements of +21.9% and +9.5% for subtasks in 1D-ARC and\n",
            "Dyck language, respectively. Codes and data are available at\n",
            "https://github.com/THUNLP-MT/symbol2language.\n",
            "Published on:  2024-01-22 07:07:06+00:00\n",
            "URL:  http://arxiv.org/abs/2401.11725v1\n",
            "------\n",
            "Paper title:  Revolutionizing Finance with LLMs: An Overview of Applications and Insights\n",
            "Summary:  In recent years, Large Language Models (LLMs) like ChatGPT have seen\n",
            "considerable advancements and have been applied in diverse fields. Built on the\n",
            "Transformer architecture, these models are trained on extensive datasets,\n",
            "enabling them to understand and generate human language effectively. In the\n",
            "financial domain, the deployment of LLMs is gaining momentum. These models are\n",
            "being utilized for automating financial report generation, forecasting market\n",
            "trends, analyzing investor sentiment, and offering personalized financial\n",
            "advice. Leveraging their natural language processing capabilities, LLMs can\n",
            "distill key insights from vast financial data, aiding institutions in making\n",
            "informed investment choices and enhancing both operational efficiency and\n",
            "customer satisfaction. In this study, we provide a comprehensive overview of\n",
            "the emerging integration of LLMs into various financial tasks. Additionally, we\n",
            "conducted holistic tests on multiple financial tasks through the combination of\n",
            "natural language instructions. Our findings show that GPT-4 effectively follow\n",
            "prompt instructions across various financial tasks. This survey and evaluation\n",
            "of LLMs in the financial domain aim to deepen the understanding of LLMs'\n",
            "current role in finance for both financial practitioners and LLM researchers,\n",
            "identify new research and application prospects, and highlight how these\n",
            "technologies can be leveraged to solve practical challenges in the finance\n",
            "industry.\n",
            "Published on:  2024-01-22 01:06:17+00:00\n",
            "URL:  http://arxiv.org/abs/2401.11641v1\n",
            "------\n",
            "Paper title:  ProLex: A Benchmark for Language Proficiency-oriented Lexical Substitution\n",
            "Summary:  Lexical Substitution discovers appropriate substitutes for a given target\n",
            "word in a context sentence. However, the task fails to consider substitutes\n",
            "that are of equal or higher proficiency than the target, an aspect that could\n",
            "be beneficial for language learners looking to improve their writing. To bridge\n",
            "this gap, we propose a new task, language proficiency-oriented lexical\n",
            "substitution. We also introduce ProLex, a novel benchmark designed to assess\n",
            "systems' ability to generate not only appropriate substitutes but also\n",
            "substitutes that demonstrate better language proficiency. Besides the\n",
            "benchmark, we propose models that can automatically perform the new task. We\n",
            "show that our best model, a Llama2-13B model fine-tuned with task-specific\n",
            "synthetic data, outperforms ChatGPT by an average of 3.2% in F-score and\n",
            "achieves comparable results with GPT-4 on ProLex.\n",
            "Published on:  2024-01-21 00:58:31+00:00\n",
            "URL:  http://arxiv.org/abs/2401.11356v1\n",
            "------\n",
            "Paper title:  Prompting Large Vision-Language Models for Compositional Reasoning\n",
            "Summary:  Vision-language models such as CLIP have shown impressive capabilities in\n",
            "encoding texts and images into aligned embeddings, enabling the retrieval of\n",
            "multimodal data in a shared embedding space. However, these embedding-based\n",
            "models still face challenges in effectively matching images and texts with\n",
            "similar visio-linguistic compositionality, as evidenced by their performance on\n",
            "the recent Winoground dataset. In this paper, we argue that this limitation\n",
            "stems from two factors: the use of single vector representations for complex\n",
            "multimodal data, and the absence of step-by-step reasoning in these\n",
            "embedding-based methods. To address this issue, we make an exploratory step\n",
            "using a novel generative method that prompts large vision-language models\n",
            "(e.g., GPT-4) to depict images and perform compositional reasoning. Our method\n",
            "outperforms other embedding-based methods on the Winoground dataset, and\n",
            "obtains further improvement of up to 10% accuracy when enhanced with the\n",
            "optimal description.\n",
            "Published on:  2024-01-20 22:04:28+00:00\n",
            "URL:  http://arxiv.org/abs/2401.11337v1\n",
            "------\n",
            "Paper title:  Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images\n",
            "Summary:  Large vision-language models (VLMs) such as GPT-4 have achieved exceptional\n",
            "performance across various multi-modal tasks. However, the deployment of VLMs\n",
            "necessitates substantial energy consumption and computational resources. Once\n",
            "attackers maliciously induce high energy consumption and latency time\n",
            "(energy-latency cost) during inference of VLMs, it will exhaust computational\n",
            "resources. In this paper, we explore this attack surface about availability of\n",
            "VLMs and aim to induce high energy-latency cost during inference of VLMs. We\n",
            "find that high energy-latency cost during inference of VLMs can be manipulated\n",
            "by maximizing the length of generated sequences. To this end, we propose\n",
            "verbose images, with the goal of crafting an imperceptible perturbation to\n",
            "induce VLMs to generate long sentences during inference. Concretely, we design\n",
            "three loss objectives. First, a loss is proposed to delay the occurrence of\n",
            "end-of-sequence (EOS) token, where EOS token is a signal for VLMs to stop\n",
            "generating further tokens. Moreover, an uncertainty loss and a token diversity\n",
            "loss are proposed to increase the uncertainty over each generated token and the\n",
            "diversity among all tokens of the whole generated sequence, respectively, which\n",
            "can break output dependency at token-level and sequence-level. Furthermore, a\n",
            "temporal weight adjustment algorithm is proposed, which can effectively balance\n",
            "these losses. Extensive experiments demonstrate that our verbose images can\n",
            "increase the length of generated sequences by 7.87 times and 8.56 times\n",
            "compared to original images on MS-COCO and ImageNet datasets, which presents\n",
            "potential challenges for various applications. Our code is available at\n",
            "https://github.com/KuofengGao/Verbose_Images.\n",
            "Published on:  2024-01-20 08:46:06+00:00\n",
            "URL:  http://arxiv.org/abs/2401.11170v1\n",
            "------\n",
            "Paper title:  Enhancing Large Language Models for Clinical Decision Support by Incorporating Clinical Practice Guidelines\n",
            "Summary:  Background Large Language Models (LLMs), enhanced with Clinical Practice\n",
            "Guidelines (CPGs), can significantly improve Clinical Decision Support (CDS).\n",
            "However, methods for incorporating CPGs into LLMs are not well studied. Methods\n",
            "We develop three distinct methods for incorporating CPGs into LLMs: Binary\n",
            "Decision Tree (BDT), Program-Aided Graph Construction (PAGC), and\n",
            "Chain-of-Thought-Few-Shot Prompting (CoT-FSP). To evaluate the effectiveness of\n",
            "the proposed methods, we create a set of synthetic patient descriptions and\n",
            "conduct both automatic and human evaluation of the responses generated by four\n",
            "LLMs: GPT-4, GPT-3.5 Turbo, LLaMA, and PaLM 2. Zero-Shot Prompting (ZSP) was\n",
            "used as the baseline method. We focus on CDS for COVID-19 outpatient treatment\n",
            "as the case study. Results All four LLMs exhibit improved performance when\n",
            "enhanced with CPGs compared to the baseline ZSP. BDT outperformed both CoT-FSP\n",
            "and PAGC in automatic evaluation. All of the proposed methods demonstrated high\n",
            "performance in human evaluation. Conclusion LLMs enhanced with CPGs demonstrate\n",
            "superior performance, as compared to plain LLMs with ZSP, in providing accurate\n",
            "recommendations for COVID-19 outpatient treatment, which also highlights the\n",
            "potential for broader applications beyond the case study.\n",
            "Published on:  2024-01-20 05:10:46+00:00\n",
            "URL:  http://arxiv.org/abs/2401.11120v1\n",
            "------\n",
            "Paper title:  BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models\n",
            "Summary:  Large language models (LLMs) are shown to benefit from chain-of-thought (COT)\n",
            "prompting, particularly when tackling tasks that require systematic reasoning\n",
            "processes. On the other hand, COT prompting also poses new vulnerabilities in\n",
            "the form of backdoor attacks, wherein the model will output unintended\n",
            "malicious content under specific backdoor-triggered conditions during\n",
            "inference. Traditional methods for launching backdoor attacks involve either\n",
            "contaminating the training dataset with backdoored instances or directly\n",
            "manipulating the model parameters during deployment. However, these approaches\n",
            "are not practical for commercial LLMs that typically operate via API access. In\n",
            "this paper, we propose BadChain, the first backdoor attack against LLMs\n",
            "employing COT prompting, which does not require access to the training dataset\n",
            "or model parameters and imposes low computational overhead. BadChain leverages\n",
            "the inherent reasoning capabilities of LLMs by inserting a backdoor reasoning\n",
            "step into the sequence of reasoning steps of the model output, thereby altering\n",
            "the final response when a backdoor trigger exists in the query prompt.\n",
            "Empirically, we show the effectiveness of BadChain for two COT strategies\n",
            "across four LLMs (Llama2, GPT-3.5, PaLM2, and GPT-4) and six complex benchmark\n",
            "tasks encompassing arithmetic, commonsense, and symbolic reasoning. Moreover,\n",
            "we show that LLMs endowed with stronger reasoning capabilities exhibit higher\n",
            "susceptibility to BadChain, exemplified by a high average attack success rate\n",
            "of 97.0% across the six benchmark tasks on GPT-4. Finally, we propose two\n",
            "defenses based on shuffling and demonstrate their overall ineffectiveness\n",
            "against BadChain. Therefore, BadChain remains a severe threat to LLMs,\n",
            "underscoring the urgency for the development of robust and effective future\n",
            "defenses.\n",
            "Published on:  2024-01-20 04:53:35+00:00\n",
            "URL:  http://arxiv.org/abs/2401.12242v1\n",
            "------\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Product_manager (to chat_manager):\n",
            "\n",
            "Based on the papers and their summaries, here are some potential applications of GPT-4 in software:\n",
            "\n",
            "1. **Meta-Prompting Framework for Breaking Down Complex Tasks**: In the work \"Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding\", GPT-4 is used to break down complex tasks into smaller ones. This could be used in project management software to help break down tasks or new feature developments into smaller, more manageable tasks.\n",
            "\n",
            "2. **Multi-Modal Chain of Thoughts Reasoning**: In the paper \"KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning\", GPT-4 is used along with knowledge graphs and multiple modalities for comprehensive understanding of multimodal tasks. This could be useful in software for data analytics and business intelligence.\n",
            "\n",
            "3. **Improving Text Generation in Language Learning Software**: The paper \"ProLex: A Benchmark for Language Proficiency-oriented Lexical Substitution\" which uses GPT-4 to generate not only appropriate substitutes but also substitutes that demonstrate better language proficiency, points towards a potential application of GPT-4 in language learning software.\n",
            "\n",
            "4. **Systems for Financial Analysis and Decision Support**: In the paper \"Revolutionizing Finance with LLMs: An Overview of Applications and Insights\", GPT-4 is deployed for automating financial report generation, forecasting market trends, analyzing investor sentiment, and offering personalized financial advice. \n",
            "\n",
            "5. **Improving Clinical Decision Support Systems**: The paper \"Enhancing Large Language Models for Clinical Decision Support by Incorporating Clinical Practice Guidelines\" employs GPT-4 to provide more accurate and informed, guidelines-based recommendations for patient treatment.\n",
            "\n",
            "Each of these potential applications utilize specific areas of GPT-4's capabilities, such as natural language processing, numeric reasoning, and multi-modal understanding. These could be leveraged in a software context to automate complex tasks, provide enhanced modelled reasoning, improve language learning, aid in financial decision-making and improve healthcare decision support systems.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "user_proxy.initiate_chat(\n",
        "    manager, message=\"Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\"\n",
        ")\n",
        "# type exit to terminate the chat"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "flaml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
