{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojKrqPJvqYTb"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wangzhenyagit/myColab/blob/main/langgraph_code_assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3900420",
      "metadata": {
        "id": "e3900420"
      },
      "outputs": [],
      "source": [
        " ! pip install -U langchain_community langchain-openai langchain-anthropic langchain langgraph bs4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38330223-d8c8-4156-82b6-93e63343bc01",
      "metadata": {
        "id": "38330223-d8c8-4156-82b6-93e63343bc01"
      },
      "source": [
        "## Docs\n",
        "\n",
        "Load [LangChain Expression Language](https://python.langchain.com/docs/expression_language/) (LCEL) docs as an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c2eb35d1-4990-47dc-a5c4-208bae588a82",
      "metadata": {
        "id": "c2eb35d1-4990-47dc-a5c4-208bae588a82"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup as Soup\n",
        "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
        "\n",
        "# Langchain docs\n",
        "url = \"https://python.langchain.com/docs/get_started/quickstart/\"\n",
        "loader = RecursiveUrlLoader(\n",
        "    url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "# Sort the list based on the URLs and get the text\n",
        "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
        "d_reversed = list(reversed(d_sorted))\n",
        "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
        "    [doc.page_content for doc in d_reversed]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = 'your_api_key_here'"
      ],
      "metadata": {
        "id": "GEh57OUGUFd7"
      },
      "id": "GEh57OUGUFd7",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KPlLaA-rUND0"
      },
      "id": "KPlLaA-rUND0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concatenated_content"
      ],
      "metadata": {
        "id": "GolOyqf9R-B3",
        "outputId": "20df8542-fc47-4e8e-be19-8c83aa501af2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "id": "GolOyqf9R-B3",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\n\\n\\nQuickstart | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubeü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏èLangGraphü¶úÔ∏èüèì LangServeSecurityGet startedQuickstartOn this pageQuickstartIn this quickstart we\\'ll show you how to:Get setup with LangChain, LangSmith and LangServeUse the most basic and common components of LangChain: prompt templates, models, and output parsersUse LangChain Expression Language, the protocol that LangChain is built on and which facilitates component chainingBuild a simple application with LangChainTrace your application with LangSmithServe your application with LangServeThat\\'s a fair amount to cover! Let\\'s dive in.Setup\\u200bJupyter Notebook\\u200bThis guide (and most of the other guides in the documentation) uses Jupyter notebooks and assumes the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because oftentimes things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.You do not NEED to go through the guide in a Jupyter Notebook, but it is recommended. See here for instructions on how to install.Installation\\u200bTo install LangChain run:PipCondapip install langchainconda install langchain -c conda-forgeFor more details, see our Installation guide.LangSmith\\u200bMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.\\nAs these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.\\nThe best way to do this is with LangSmith.Note that LangSmith is not needed, but it is helpful.\\nIf you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:export LANGCHAIN_TRACING_V2=\"true\"export LANGCHAIN_API_KEY=\"...\"Building with LangChain\\u200bLangChain enables building application that connect external sources of data and computation to LLMs.\\nIn this quickstart, we will walk through a few different ways of doing that.\\nWe will start with a simple LLM chain, which just relies on information in the prompt template to respond.\\nNext, we will build a retrieval chain, which fetches data from a separate database and passes that into the prompt template.\\nWe will then add in chat history, to create a conversation retrieval chain. This allows you to interact in a chat manner with this LLM, so it remembers previous questions.\\nFinally, we will build an agent - which utilizes an LLM to determine whether or not it needs to fetch data to answer questions.\\nWe will cover these at a high level, but there are lot of details to all of these!\\nWe will link to relevant docs.LLM Chain\\u200bWe\\'ll show how to use models available via API, like OpenAI, and local open source models, using integrations like Ollama.OpenAILocal (using Ollama)AnthropicCohereFirst we\\'ll need to import the LangChain x OpenAI integration package.pip install langchain-openaiAccessing the API requires an API key, which you can get by creating an account and heading here. Once we have a key we\\'ll want to set it as an environment variable by running:export OPENAI_API_KEY=\"...\"We can then initialize the model:from langchain_openai import ChatOpenAIllm = ChatOpenAI()API Reference:ChatOpenAIIf you\\'d prefer not to set an environment variable you can pass the key in directly via the api_key named parameter when initiating the OpenAI LLM class:from langchain_openai import ChatOpenAIllm = ChatOpenAI(api_key=\"...\")API Reference:ChatOpenAIOllama allows you to run open-source large language models, such as Llama 2, locally.First, follow these instructions to set up and run a local Ollama instance:DownloadFetch a model via ollama pull llama2Then, make sure the Ollama server is running. After that, you can do:from langchain_community.llms import Ollamallm = Ollama(model=\"llama2\")API Reference:OllamaFirst we\\'ll need to import the LangChain x Anthropic package.pip install langchain-anthropicAccessing the API requires an API key, which you can get by creating an account here. Once we have a key we\\'ll want to set it as an environment variable by running:export ANTHROPIC_API_KEY=\"...\"We can then initialize the model:from langchain_anthropic import ChatAnthropicllm = ChatAnthropic(model=\"claude-3-sonnet-20240229\", temperature=0.2, max_tokens=1024)API Reference:ChatAnthropicIf you\\'d prefer not to set an environment variable you can pass the key in directly via the api_key named parameter when initiating the Anthropic Chat Model class:llm = ChatAnthropic(api_key=\"...\")First we\\'ll need to import the Cohere SDK package.pip install langchain-cohereAccessing the API requires an API key, which you can get by creating an account and heading here. Once we have a key we\\'ll want to set it as an environment variable by running:export COHERE_API_KEY=\"...\"We can then initialize the model:from langchain_cohere import ChatCoherellm = ChatCohere()API Reference:ChatCohereIf you\\'d prefer not to set an environment variable you can pass the key in directly via the cohere_api_key named parameter when initiating the Cohere LLM class:from langchain_cohere import ChatCoherellm = ChatCohere(cohere_api_key=\"...\")API Reference:ChatCohereOnce you\\'ve installed and initialized the LLM of your choice, we can try using it!\\nLet\\'s ask it what LangSmith is - this is something that wasn\\'t present in the training data so it shouldn\\'t have a very good response.llm.invoke(\"how can langsmith help with testing?\")We can also guide its response with a prompt template.\\nPrompt templates convert raw user input to better input to the LLM.from langchain_core.prompts import ChatPromptTemplateprompt = ChatPromptTemplate.from_messages([    (\"system\", \"You are a world class technical documentation writer.\"),    (\"user\", \"{input}\")])API Reference:ChatPromptTemplateWe can now combine these into a simple LLM chain:chain = prompt | llm We can now invoke it and ask the same question. It still won\\'t know the answer, but it should respond in a more proper tone for a technical writer!chain.invoke({\"input\": \"how can langsmith help with testing?\"})The output of a ChatModel (and therefore, of this chain) is a message. However, it\\'s often much more convenient to work with strings. Let\\'s add a simple output parser to convert the chat message to a string.from langchain_core.output_parsers import StrOutputParseroutput_parser = StrOutputParser()API Reference:StrOutputParserWe can now add this to the previous chain:chain = prompt | llm | output_parserWe can now invoke it and ask the same question. The answer will now be a string (rather than a ChatMessage).chain.invoke({\"input\": \"how can langsmith help with testing?\"})Diving Deeper\\u200bWe\\'ve now successfully set up a basic LLM chain. We only touched on the basics of prompts, models, and output parsers - for a deeper dive into everything mentioned here, see this section of documentation.Retrieval Chain\\u200bTo properly answer the original question (\"how can langsmith help with testing?\"), we need to provide additional context to the LLM.\\nWe can do this via retrieval.\\nRetrieval is useful when you have too much data to pass to the LLM directly.\\nYou can then use a retriever to fetch only the most relevant pieces and pass those in.In this process, we will look up relevant documents from a Retriever and then pass them into the prompt.\\nA Retriever can be backed by anything - a SQL table, the internet, etc - but in this instance we will populate a vector store and use that as a retriever. For more information on vectorstores, see this documentation.First, we need to load the data that we want to index. To do this, we will use the WebBaseLoader. This requires installing BeautifulSoup:pip install beautifulsoup4After that, we can import and use WebBaseLoader.from langchain_community.document_loaders import WebBaseLoaderloader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")docs = loader.load()API Reference:WebBaseLoaderNext, we need to index it into a vectorstore. This requires a few components, namely an embedding model and a vectorstore.For embedding models, we once again provide examples for accessing via API or by running local models.OpenAI (API)Local (using Ollama)Cohere (API)Make sure you have the `langchain_openai` package installed an the appropriate environment variables set (these are the same as needed for the LLM).from langchain_openai import OpenAIEmbeddingsembeddings = OpenAIEmbeddings()API Reference:OpenAIEmbeddingsMake sure you have Ollama running (same set up as with the LLM).from langchain_community.embeddings import OllamaEmbeddingsembeddings = OllamaEmbeddings()API Reference:OllamaEmbeddingsMake sure you have the cohere package installed and the appropriate environment variables set (these are the same as needed for the LLM).from langchain_cohere.embeddings import CohereEmbeddingsembeddings = CohereEmbeddings()API Reference:CohereEmbeddingsNow, we can use this embedding model to ingest documents into a vectorstore.\\nWe will use a simple local vectorstore, FAISS, for simplicity\\'s sake.First we need to install the required packages for that:pip install faiss-cpuThen we can build our index:from langchain_community.vectorstores import FAISSfrom langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter()documents = text_splitter.split_documents(docs)vector = FAISS.from_documents(documents, embeddings)API Reference:FAISSRecursiveCharacterTextSplitterNow that we have this data indexed in a vectorstore, we will create a retrieval chain.\\nThis chain will take an incoming question, look up relevant documents, then pass those documents along with the original question into an LLM and ask it to answer the original question.First, let\\'s set up the chain that takes a question and the retrieved documents and generates an answer.from langchain.chains.combine_documents import create_stuff_documents_chainprompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:<context>{context}</context>Question: {input}\"\"\")document_chain = create_stuff_documents_chain(llm, prompt)API Reference:create_stuff_documents_chainIf we wanted to, we could run this ourselves by passing in documents directly:from langchain_core.documents import Documentdocument_chain.invoke({    \"input\": \"how can langsmith help with testing?\",    \"context\": [Document(page_content=\"langsmith can let you visualize test results\")]})API Reference:DocumentHowever, we want the documents to first come from the retriever we just set up.\\nThat way, we can use the retriever to dynamically select the most relevant documents and pass those in for a given question.from langchain.chains import create_retrieval_chainretriever = vector.as_retriever()retrieval_chain = create_retrieval_chain(retriever, document_chain)API Reference:create_retrieval_chainWe can now invoke this chain. This returns a dictionary - the response from the LLM is in the answer keyresponse = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})print(response[\"answer\"])# LangSmith offers several features that can help with testing:...This answer should be much more accurate!Diving Deeper\\u200bWe\\'ve now successfully set up a basic retrieval chain. We only touched on the basics of retrieval - for a deeper dive into everything mentioned here, see this section of documentation.Conversation Retrieval Chain\\u200bThe chain we\\'ve created so far can only answer single questions. One of the main types of LLM applications that people are building are chat bots. So how do we turn this chain into one that can answer follow up questions?We can still use the create_retrieval_chain function, but we need to change two things:The retrieval method should now not just work on the most recent input, but rather should take the whole history into account.The final LLM chain should likewise take the whole history into accountUpdating RetrievalIn order to update retrieval, we will create a new chain. This chain will take in the most recent input (input) and the conversation history (chat_history) and use an LLM to generate a search query.from langchain.chains import create_history_aware_retrieverfrom langchain_core.prompts import MessagesPlaceholder# First we need a prompt that we can pass into an LLM to generate this search queryprompt = ChatPromptTemplate.from_messages([    MessagesPlaceholder(variable_name=\"chat_history\"),    (\"user\", \"{input}\"),    (\"user\", \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\")])retriever_chain = create_history_aware_retriever(llm, retriever, prompt)API Reference:create_history_aware_retrieverMessagesPlaceholderWe can test this out by passing in an instance where the user asks a follow-up question.from langchain_core.messages import HumanMessage, AIMessagechat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]retriever_chain.invoke({    \"chat_history\": chat_history,    \"input\": \"Tell me how\"})API Reference:HumanMessageAIMessageYou should see that this returns documents about testing in LangSmith. This is because the LLM generated a new query, combining the chat history with the follow-up question.Now that we have this new retriever, we can create a new chain to continue the conversation with these retrieved documents in mind.prompt = ChatPromptTemplate.from_messages([    (\"system\", \"Answer the user\\'s questions based on the below context:\\\\n\\\\n{context}\"),    MessagesPlaceholder(variable_name=\"chat_history\"),    (\"user\", \"{input}\"),])document_chain = create_stuff_documents_chain(llm, prompt)retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)We can now test this out end-to-end:chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]retrieval_chain.invoke({    \"chat_history\": chat_history,    \"input\": \"Tell me how\"})We can see that this gives a coherent answer - we\\'ve successfully turned our retrieval chain into a chatbot!Agent\\u200bWe\\'ve so far created examples of chains - where each step is known ahead of time.\\nThe final thing we will create is an agent - where the LLM decides what steps to take.NOTE: for this example we will only show how to create an agent using OpenAI models, as local models are not reliable enough yet.One of the first things to do when building an agent is to decide what tools it should have access to.\\nFor this example, we will give the agent access to two tools:The retriever we just created. This will let it easily answer questions about LangSmithA search tool. This will let it easily answer questions that require up-to-date information.First, let\\'s set up a tool for the retriever we just created:from langchain.tools.retriever import create_retriever_toolretriever_tool = create_retriever_tool(    retriever,    \"langsmith_search\",    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",)API Reference:create_retriever_toolThe search tool that we will use is Tavily. This will require an API key (they have generous free tier). After creating it on their platform, you need to set it as an environment variable:export TAVILY_API_KEY=...If you do not want to set up an API key, you can skip creating this tool.from langchain_community.tools.tavily_search import TavilySearchResultssearch = TavilySearchResults()API Reference:TavilySearchResultsWe can now create a list of the tools we want to work with:tools = [retriever_tool, search]Now that we have the tools, we can create an agent to use them. We will go over this pretty quickly - for a deeper dive into what exactly is going on, check out the Agent\\'s Getting Started documentationInstall langchain hub firstpip install langchainhubInstall the langchain-openai package\\nTo interact with OpenAI we need to use langchain-openai which connects with OpenAI SDK[https://github.com/langchain-ai/langchain/tree/master/libs/partners/openai].  pip install langchain-openaiNow we can use it to get a predefined promptfrom langchain_openai import ChatOpenAIfrom langchain import hubfrom langchain.agents import create_openai_functions_agentfrom langchain.agents import AgentExecutor# Get the prompt to use - you can modify this!prompt = hub.pull(\"hwchase17/openai-functions-agent\")# You need to set OPENAI_API_KEY environment variable or pass it as argument `api_key`.llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)agent = create_openai_functions_agent(llm, tools, prompt)agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)API Reference:ChatOpenAIcreate_openai_functions_agentAgentExecutorWe can now invoke the agent and see how it responds! We can ask it questions about LangSmith:agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})We can ask it about the weather:agent_executor.invoke({\"input\": \"what is the weather in SF?\"})We can have conversations with it:chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]agent_executor.invoke({    \"chat_history\": chat_history,    \"input\": \"Tell me how\"})Diving Deeper\\u200bWe\\'ve now successfully set up a basic agent. We only touched on the basics of agents - for a deeper dive into everything mentioned here, see this section of documentation.Serving with LangServe\\u200bNow that we\\'ve built an application, we need to serve it. That\\'s where LangServe comes in.\\nLangServe helps developers deploy LangChain chains as a REST API. You do not need to use LangServe to use LangChain, but in this guide we\\'ll show how you can deploy your app with LangServe.While the first part of this guide was intended to be run in a Jupyter Notebook, we will now move out of that. We will be creating a Python file and then interacting with it from the command line.Install with:pip install \"langserve[all]\"Server\\u200bTo create a server for our application we\\'ll make a serve.py file. This will contain our logic for serving our application. It consists of three things:The definition of our chain that we just built aboveOur FastAPI appA definition of a route from which to serve the chain, which is done with langserve.add_routes#!/usr/bin/env pythonfrom typing import Listfrom fastapi import FastAPIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_openai import OpenAIEmbeddingsfrom langchain_community.vectorstores import FAISSfrom langchain_text_splitters import RecursiveCharacterTextSplitterfrom langchain.tools.retriever import create_retriever_toolfrom langchain_community.tools.tavily_search import TavilySearchResultsfrom langchain import hubfrom langchain.agents import create_openai_functions_agentfrom langchain.agents import AgentExecutorfrom langchain.pydantic_v1 import BaseModel, Fieldfrom langchain_core.messages import BaseMessagefrom langserve import add_routes# 1. Load Retrieverloader = WebBaseLoader(\"https://docs.smith.langchain.com/user_guide\")docs = loader.load()text_splitter = RecursiveCharacterTextSplitter()documents = text_splitter.split_documents(docs)embeddings = OpenAIEmbeddings()vector = FAISS.from_documents(documents, embeddings)retriever = vector.as_retriever()# 2. Create Toolsretriever_tool = create_retriever_tool(    retriever,    \"langsmith_search\",    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",)search = TavilySearchResults()tools = [retriever_tool, search]# 3. Create Agentprompt = hub.pull(\"hwchase17/openai-functions-agent\")llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)agent = create_openai_functions_agent(llm, tools, prompt)agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)# 4. App definitionapp = FastAPI(  title=\"LangChain Server\",  version=\"1.0\",  description=\"A simple API server using LangChain\\'s Runnable interfaces\",)# 5. Adding chain route# We need to add these input/output schemas because the current AgentExecutor# is lacking in schemas.class Input(BaseModel):    input: str    chat_history: List[BaseMessage] = Field(        ...,        extra={\"widget\": {\"type\": \"chat\", \"input\": \"location\"}},    )class Output(BaseModel):    output: stradd_routes(    app,    agent_executor.with_types(input_type=Input, output_type=Output),    path=\"/agent\",)if __name__ == \"__main__\":    import uvicorn    uvicorn.run(app, host=\"localhost\", port=8000)API Reference:ChatPromptTemplateChatOpenAIWebBaseLoaderOpenAIEmbeddingsFAISSRecursiveCharacterTextSplittercreate_retriever_toolTavilySearchResultscreate_openai_functions_agentAgentExecutorBaseMessageAnd that\\'s it! If we execute this file:python serve.pywe should see our chain being served at localhost:8000.Playground\\u200bEvery LangServe service comes with a simple built-in UI for configuring and invoking the application with streaming output and visibility into intermediate steps.\\nHead to http://localhost:8000/agent/playground/ to try it out! Pass in the same question as before - \"how can langsmith help with testing?\" - and it should respond same as before.Client\\u200bNow let\\'s set up a client for programmatically interacting with our service. We can easily do this with the [langserve.RemoteRunnable](/docs/langserve#client).\\nUsing this, we can interact with the served chain as if it were running client-side.from langserve import RemoteRunnableremote_chain = RemoteRunnable(\"http://localhost:8000/agent/\")remote_chain.invoke({    \"input\": \"how can langsmith help with testing?\",    \"chat_history\": []  # Providing an empty list as this is the first call})To learn more about the many other features of LangServe head here.Next steps\\u200bWe\\'ve touched on how to build an application with LangChain, how to trace it with LangSmith, and how to serve it with LangServe.\\nThere are a lot more features in all three of these than we can cover here.\\nTo continue on your journey, we recommend you read the following (in order):All of these features are backed by LangChain Expression Language (LCEL) - a way to chain these components together. Check out that documentation to better understand how to create custom chains.Model IO covers more details of prompts, LLMs, and output parsers.Retrieval covers more details of everything related to retrievalAgents covers details of everything related to agentsExplore common end-to-end use cases and template applicationsRead up on LangSmith, the platform for debugging, testing, monitoring and moreLearn more about serving your applications with LangServeHelp us out by providing feedback on this documentation page:PreviousIntroductionNextInstallationSetupJupyter NotebookInstallationLangSmithBuilding with LangChainLLM ChainDiving DeeperRetrieval ChainDiving DeeperConversation Retrieval ChainAgentDiving DeeperServing with LangServeServerPlaygroundClientNext stepsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "3ba3df70-f6b4-4ea5-a210-e10944960bc6",
      "metadata": {
        "id": "3ba3df70-f6b4-4ea5-a210-e10944960bc6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = 'sk-proj-yuCEDh0twTz70cRVaItoT3BlbkFJ1MWMxw288GpXUAr1vRK3'\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "### OpenAI\n",
        "\n",
        "# Grader prompt\n",
        "code_gen_prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\",\"\"\"You are a coding assistant with expertise in LangChain. \\n\n",
        "    Here is a full set of LangChain documentation:  \\n ------- \\n  {context} \\n ------- \\n Answer the user\n",
        "    question based on the above provided documentation. Ensure any code you provide can be executed \\n\n",
        "    with all required imports and variables defined. Structure your answer with a description of the code solution. \\n\n",
        "    If code use ChatOpenAI class, do not use the api_key parameter. \\n\n",
        "    Then list the imports. And finally list the functioning code block. Here is the user question:\"\"\"),\n",
        "    (\"placeholder\", \"{messages}\")]\n",
        ")\n",
        "\n",
        "# Data model\n",
        "class code(BaseModel):\n",
        "    \"\"\"Code output\"\"\"\n",
        "\n",
        "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
        "    imports: str = Field(description=\"Code block import statements\")\n",
        "    code: str = Field(description=\"Code block not including import statements\")\n",
        "    description = \"Schema for code solutions to questions about LCEL.\"\n",
        "\n",
        "expt_llm = \"gpt-4-1106-preview\"\n",
        "llm = ChatOpenAI(temperature=0, model=expt_llm)\n",
        "code_gen_chain = code_gen_prompt | llm.with_structured_output(code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "9f14750f-dddc-485b-ba29-5392cdf4ba43",
      "metadata": {
        "scrolled": true,
        "id": "9f14750f-dddc-485b-ba29-5392cdf4ba43"
      },
      "outputs": [],
      "source": [
        "question = \"How do I ask simple question use langchain?\"\n",
        "solution = code_gen_chain.invoke({\"context\":concatenated_content,\"messages\":[(\"user\",question)]})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "solution.code"
      ],
      "metadata": {
        "id": "GtkJ204qWxSt",
        "outputId": "7521f969-c77b-4c5a-ef0a-6373b678c22c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "id": "GtkJ204qWxSt",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Initialize the OpenAI model\\nllm = ChatOpenAI(api_key=\\'your_openai_api_key\\')\\n\\n# Define a prompt template\\nprompt = ChatPromptTemplate.from_messages([\\n    (\"system\", \"You are a world class technical documentation writer.\"),\\n    (\"user\", \"{input}\")\\n])\\n\\n# Combine the prompt with the model\\nchain = prompt | llm\\n\\n# Add an output parser to convert the chat message to a string\\noutput_parser = StrOutputParser()\\nchain = chain | output_parser\\n\\n# Invoke the chain with a simple question\\nresponse = chain.invoke({\"input\": \"How can LangSmith help with testing?\"})\\nprint(response)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "131f2055-2f64-4d19-a3d1-2d3cb8b42894",
      "metadata": {
        "id": "131f2055-2f64-4d19-a3d1-2d3cb8b42894"
      },
      "source": [
        "## State\n",
        "\n",
        "Our state is a dict that will contain keys (errors, question, code generation) relevant to code generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "c185f1a2-e943-4bed-b833-4243c9c64092",
      "metadata": {
        "id": "c185f1a2-e943-4bed-b833-4243c9c64092"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, TypedDict, List\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        error : Binary flag for control flow to indicate whether test error was tripped\n",
        "        messages : With user question, error messages, reasoning\n",
        "        generation : Code solution\n",
        "        iterations : Number of tries\n",
        "    \"\"\"\n",
        "\n",
        "    error : str\n",
        "    messages : List\n",
        "    generation : str\n",
        "    iterations : int"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64454465-26a3-40de-ad85-bcf59a2c3086",
      "metadata": {
        "id": "64454465-26a3-40de-ad85-bcf59a2c3086"
      },
      "source": [
        "## Graph\n",
        "\n",
        "Our graph lays out the logical flow shown in the figure above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "b70e8301-63ae-4f7e-ad8f-c9a052fe3566",
      "metadata": {
        "id": "b70e8301-63ae-4f7e-ad8f-c9a052fe3566"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "### Parameter\n",
        "\n",
        "# Max tries\n",
        "max_iterations = 3\n",
        "# Reflect\n",
        "# flag = 'reflect'\n",
        "flag = 'do not reflect'\n",
        "\n",
        "### Nodes\n",
        "\n",
        "def generate(state: GraphState):\n",
        "    \"\"\"\n",
        "    Generate a code solution\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---GENERATING CODE SOLUTION---\")\n",
        "\n",
        "    # State\n",
        "    messages = state[\"messages\"]\n",
        "    iterations = state[\"iterations\"]\n",
        "    error = state[\"error\"]\n",
        "\n",
        "    # We have been routed back to generation with an error\n",
        "    if error == \"yes\":\n",
        "        messages += [(\"user\",\"Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:\")]\n",
        "\n",
        "    # Solution\n",
        "    code_solution = code_gen_chain.invoke({\"context\": concatenated_content, \"messages\" : messages})\n",
        "    messages += [(\"assistant\",f\"{code_solution.prefix} \\n Imports: {code_solution.imports} \\n Code: {code_solution.code}\")]\n",
        "\n",
        "    # Increment\n",
        "    iterations = iterations + 1\n",
        "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
        "\n",
        "def code_check(state: GraphState):\n",
        "    \"\"\"\n",
        "    Check code\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, error\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECKING CODE---\")\n",
        "\n",
        "    # State\n",
        "    messages = state[\"messages\"]\n",
        "    code_solution = state[\"generation\"]\n",
        "    iterations = state[\"iterations\"]\n",
        "\n",
        "    # Get solution components\n",
        "    prefix = code_solution.prefix\n",
        "    imports = code_solution.imports\n",
        "    code = code_solution.code\n",
        "\n",
        "    # Check imports\n",
        "    try:\n",
        "        exec(imports)\n",
        "    except Exception as e:\n",
        "        print(\"---CODE IMPORT CHECK: FAILED---\")\n",
        "        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n",
        "        messages += error_message\n",
        "        return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations, \"error\": \"yes\"}\n",
        "\n",
        "    # Check execution\n",
        "    try:\n",
        "        exec(imports + \"\\n\" + code)\n",
        "    except Exception as e:\n",
        "        print(\"---CODE BLOCK CHECK: FAILED---\")\n",
        "        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n",
        "        messages += error_message\n",
        "        return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations, \"error\": \"yes\"}\n",
        "\n",
        "    # No errors\n",
        "    print(\"---NO CODE TEST FAILURES---\")\n",
        "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations, \"error\": \"no\"}\n",
        "\n",
        "def reflect(state: GraphState):\n",
        "    \"\"\"\n",
        "    Reflect on errors\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---GENERATING CODE SOLUTION---\")\n",
        "\n",
        "    # State\n",
        "    messages = state[\"messages\"]\n",
        "    iterations = state[\"iterations\"]\n",
        "    code_solution = state[\"generation\"]\n",
        "\n",
        "    # Prompt reflection\n",
        "    reflection_message = [(\"user\", \"\"\"You tried to solve this problem and failed a unit test. Reflect on this failure\n",
        "                                    given the provided documentation. Write a few key suggestions based on the\n",
        "                                    documentation to avoid making this mistake again.\"\"\")]\n",
        "\n",
        "    # Add reflection\n",
        "    reflections = code_gen_chain.invoke({\"context\"  : concatenated_content, \"messages\" : messages})\n",
        "    messages += [(\"assistant\" , f\"Here are reflections on the error: {reflections}\")]\n",
        "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
        "\n",
        "### Edges\n",
        "\n",
        "def decide_to_finish(state: GraphState):\n",
        "    \"\"\"\n",
        "    Determines whether to finish.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Next node to call\n",
        "    \"\"\"\n",
        "    error = state[\"error\"]\n",
        "    iterations = state[\"iterations\"]\n",
        "\n",
        "    if error == \"no\" or iterations == max_iterations:\n",
        "        print(\"---DECISION: FINISH---\")\n",
        "        return \"end\"\n",
        "    else:\n",
        "        print(\"---DECISION: RE-TRY SOLUTION---\")\n",
        "        if flag == 'reflect':\n",
        "            return \"reflect\"\n",
        "        else:\n",
        "            return \"generate\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "f66b4e00-4731-42c8-bc38-72dd0ff7c92c",
      "metadata": {
        "id": "f66b4e00-4731-42c8-bc38-72dd0ff7c92c"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"generate\", generate)  # generation solution\n",
        "workflow.add_node(\"check_code\", code_check)  # check code\n",
        "workflow.add_node(\"reflect\", reflect)  # reflect\n",
        "\n",
        "# Build graph\n",
        "workflow.set_entry_point(\"generate\")\n",
        "workflow.add_edge(\"generate\", \"check_code\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"check_code\",\n",
        "    decide_to_finish,\n",
        "    {\n",
        "        \"end\": END,\n",
        "        \"reflect\": \"reflect\",\n",
        "        \"generate\": \"generate\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"reflect\", \"generate\")\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "9bcaafe4-ddcf-4fab-8620-2d9b6c508f98",
      "metadata": {
        "id": "9bcaafe4-ddcf-4fab-8620-2d9b6c508f98",
        "outputId": "fbabcedf-24a0-47ac-ac2f-06a21f0eb8ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "A large language model is a type of artificial intelligence model that has been trained on massive amounts of text data to understand and generate human-like language. These models are typically based on deep learning techniques, such as neural networks, and have the capability to process and generate text at a large scale. Examples of large language models include GPT-3 (Generative Pre-trained Transformer 3) developed by OpenAI and BERT (Bidirectional Encoder Representations from Transformers) developed by Google.\n",
            "---NO CODE TEST FAILURES---\n",
            "---DECISION: FINISH---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'error': 'no',\n",
              " 'messages': [('user',\n",
              "   'How can I use langchain ask question: what is large language model ?'),\n",
              "  ('assistant',\n",
              "   'Using LangChain to ask a question about large language models involves setting up the necessary components such as the LLM (Large Language Model) integration, prompt templates, and output parsers. The following code demonstrates how to initialize a ChatOpenAI instance, create a prompt template, and invoke the LLM to ask the question \\'What is a large language model?\\'. The code assumes that the necessary packages have been installed and the OPENAI_API_KEY environment variable has been set. \\n Imports: from langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser \\n Code: # Initialize the ChatOpenAI instance\\nllm = ChatOpenAI()\\n\\n# Create a prompt template\\nprompt = ChatPromptTemplate.from_messages([\\n    (\"system\", \"You are an AI knowledgeable about language models.\"),\\n    (\"user\", \"{input}\")\\n])\\n\\n# Combine the prompt and the LLM into a chain\\nchain = prompt | llm\\n\\n# Add an output parser to convert the chat message to a string\\noutput_parser = StrOutputParser()\\nchain = chain | output_parser\\n\\n# Invoke the chain with the question\\nresponse = chain.invoke({\"input\": \"What is a large language model?\"})\\n\\n# Print the response\\nprint(response)')],\n",
              " 'generation': code(prefix=\"Using LangChain to ask a question about large language models involves setting up the necessary components such as the LLM (Large Language Model) integration, prompt templates, and output parsers. The following code demonstrates how to initialize a ChatOpenAI instance, create a prompt template, and invoke the LLM to ask the question 'What is a large language model?'. The code assumes that the necessary packages have been installed and the OPENAI_API_KEY environment variable has been set.\", imports='from langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_core.output_parsers import StrOutputParser', code='# Initialize the ChatOpenAI instance\\nllm = ChatOpenAI()\\n\\n# Create a prompt template\\nprompt = ChatPromptTemplate.from_messages([\\n    (\"system\", \"You are an AI knowledgeable about language models.\"),\\n    (\"user\", \"{input}\")\\n])\\n\\n# Combine the prompt and the LLM into a chain\\nchain = prompt | llm\\n\\n# Add an output parser to convert the chat message to a string\\noutput_parser = StrOutputParser()\\nchain = chain | output_parser\\n\\n# Invoke the chain with the question\\nresponse = chain.invoke({\"input\": \"What is a large language model?\"})\\n\\n# Print the response\\nprint(response)', description='Schema for code solutions to questions about LCEL.'),\n",
              " 'iterations': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "question = \"How can I use langchain ask question: what is large language model ?\"\n",
        "app.invoke({\"messages\":[(\"user\",question)],\"iterations\":0})"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
