{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wangzhenyagit/myColab/blob/main/langgraph_code_assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3900420",
      "metadata": {
        "id": "e3900420"
      },
      "outputs": [],
      "source": [
        " ! pip install -U langchain_community langchain-openai langchain langgraph bs4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38330223-d8c8-4156-82b6-93e63343bc01",
      "metadata": {
        "id": "38330223-d8c8-4156-82b6-93e63343bc01"
      },
      "source": [
        "## Docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "c2eb35d1-4990-47dc-a5c4-208bae588a82",
      "metadata": {
        "id": "c2eb35d1-4990-47dc-a5c4-208bae588a82"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup as Soup\n",
        "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
        "\n",
        "# Langchain docs\n",
        "url = \"https://python.langchain.com/docs/get_started/quickstart/\"\n",
        "loader = RecursiveUrlLoader(\n",
        "    url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "# Sort the list based on the URLs and get the text\n",
        "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
        "d_reversed = list(reversed(d_sorted))\n",
        "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
        "    [doc.page_content for doc in d_reversed]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = 'your_api_key_here'"
      ],
      "metadata": {
        "id": "GEh57OUGUFd7"
      },
      "id": "GEh57OUGUFd7",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "3ba3df70-f6b4-4ea5-a210-e10944960bc6",
      "metadata": {
        "id": "3ba3df70-f6b4-4ea5-a210-e10944960bc6"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "### OpenAI\n",
        "\n",
        "# Grader prompt\n",
        "code_gen_prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\",\"\"\"You are a coding assistant with expertise in LangChain. \\n\n",
        "    Here is a full set of LangChain documentation:  \\n ------- \\n  {context} \\n ------- \\n Answer the user\n",
        "    question based on the above provided documentation. Ensure any code you provide can be executed \\n\n",
        "    with all required imports and variables defined. Structure your answer with a description of the code solution. \\n\n",
        "    If code use ChatOpenAI class, do not use the api_key parameter. \\n\n",
        "    Then list the imports. And finally list the functioning code block. Here is the user question:\"\"\"),\n",
        "    (\"placeholder\", \"{messages}\")]\n",
        ")\n",
        "\n",
        "# Data model\n",
        "class code(BaseModel):\n",
        "    \"\"\"Code output\"\"\"\n",
        "\n",
        "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
        "    imports: str = Field(description=\"Code block import statements\")\n",
        "    code: str = Field(description=\"Code block not including import statements\")\n",
        "    description = \"Schema for code solutions to questions about LCEL.\"\n",
        "\n",
        "expt_llm = \"gpt-4-1106-preview\"\n",
        "llm = ChatOpenAI(temperature=0, model=expt_llm)\n",
        "code_gen_chain = code_gen_prompt | llm.with_structured_output(code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "9f14750f-dddc-485b-ba29-5392cdf4ba43",
      "metadata": {
        "scrolled": true,
        "id": "9f14750f-dddc-485b-ba29-5392cdf4ba43"
      },
      "outputs": [],
      "source": [
        "question = \"How do I ask simple question use langchain?\"\n",
        "solution = code_gen_chain.invoke({\"context\":concatenated_content,\"messages\":[(\"user\",question)]})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "solution.code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "GtkJ204qWxSt",
        "outputId": "7521f969-c77b-4c5a-ef0a-6373b678c22c"
      },
      "id": "GtkJ204qWxSt",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# Initialize the OpenAI model\\nllm = ChatOpenAI(api_key=\\'your_openai_api_key\\')\\n\\n# Define a prompt template\\nprompt = ChatPromptTemplate.from_messages([\\n    (\"system\", \"You are a world class technical documentation writer.\"),\\n    (\"user\", \"{input}\")\\n])\\n\\n# Combine the prompt with the model\\nchain = prompt | llm\\n\\n# Add an output parser to convert the chat message to a string\\noutput_parser = StrOutputParser()\\nchain = chain | output_parser\\n\\n# Invoke the chain with a simple question\\nresponse = chain.invoke({\"input\": \"How can LangSmith help with testing?\"})\\nprint(response)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "131f2055-2f64-4d19-a3d1-2d3cb8b42894",
      "metadata": {
        "id": "131f2055-2f64-4d19-a3d1-2d3cb8b42894"
      },
      "source": [
        "## State\n",
        "\n",
        "Our state is a dict that will contain keys (errors, question, code generation) relevant to code generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "c185f1a2-e943-4bed-b833-4243c9c64092",
      "metadata": {
        "id": "c185f1a2-e943-4bed-b833-4243c9c64092"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, TypedDict, List\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        error : Binary flag for control flow to indicate whether test error was tripped\n",
        "        messages : With user question, error messages, reasoning\n",
        "        generation : Code solution\n",
        "        iterations : Number of tries\n",
        "    \"\"\"\n",
        "\n",
        "    error : str\n",
        "    messages : List\n",
        "    generation : str\n",
        "    iterations : int"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64454465-26a3-40de-ad85-bcf59a2c3086",
      "metadata": {
        "id": "64454465-26a3-40de-ad85-bcf59a2c3086"
      },
      "source": [
        "## Graph\n",
        "\n",
        "Our graph lays out the logical flow shown in the figure above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "b70e8301-63ae-4f7e-ad8f-c9a052fe3566",
      "metadata": {
        "id": "b70e8301-63ae-4f7e-ad8f-c9a052fe3566"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "### Parameter\n",
        "\n",
        "# Max tries\n",
        "max_iterations = 3\n",
        "\n",
        "### Nodes\n",
        "\n",
        "def generate(state: GraphState):\n",
        "    \"\"\"\n",
        "    Generate a code solution\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---GENERATING CODE SOLUTION---\")\n",
        "\n",
        "    # State\n",
        "    messages = state[\"messages\"]\n",
        "    iterations = state[\"iterations\"]\n",
        "    error = state[\"error\"]\n",
        "\n",
        "    # We have been routed back to generation with an error\n",
        "    if error == \"yes\":\n",
        "        messages += [(\"user\",\"Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:\")]\n",
        "\n",
        "    # Solution\n",
        "    code_solution = code_gen_chain.invoke({\"context\": concatenated_content, \"messages\" : messages})\n",
        "    messages += [(\"assistant\",f\"{code_solution.prefix} \\n Imports: {code_solution.imports} \\n Code: {code_solution.code}\")]\n",
        "\n",
        "    # Increment\n",
        "    iterations = iterations + 1\n",
        "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
        "\n",
        "def code_check(state: GraphState):\n",
        "    \"\"\"\n",
        "    Check code\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, error\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECKING CODE---\")\n",
        "\n",
        "    # State\n",
        "    messages = state[\"messages\"]\n",
        "    code_solution = state[\"generation\"]\n",
        "    iterations = state[\"iterations\"]\n",
        "\n",
        "    # Get solution components\n",
        "    prefix = code_solution.prefix\n",
        "    imports = code_solution.imports\n",
        "    code = code_solution.code\n",
        "\n",
        "    # Check imports\n",
        "    try:\n",
        "        exec(imports)\n",
        "    except Exception as e:\n",
        "        print(\"---CODE IMPORT CHECK: FAILED---\")\n",
        "        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n",
        "        messages += error_message\n",
        "        return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations, \"error\": \"yes\"}\n",
        "\n",
        "    # Check execution\n",
        "    try:\n",
        "        exec(imports + \"\\n\" + code)\n",
        "    except Exception as e:\n",
        "        print(\"---CODE BLOCK CHECK: FAILED---\")\n",
        "        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n",
        "        messages += error_message\n",
        "        return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations, \"error\": \"yes\"}\n",
        "\n",
        "    # No errors\n",
        "    print(\"---NO CODE TEST FAILURES---\")\n",
        "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations, \"error\": \"no\"}\n",
        "\n",
        "### Edges\n",
        "\n",
        "def decide_to_finish(state: GraphState):\n",
        "    \"\"\"\n",
        "    Determines whether to finish.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Next node to call\n",
        "    \"\"\"\n",
        "    error = state[\"error\"]\n",
        "    iterations = state[\"iterations\"]\n",
        "\n",
        "    if error == \"no\" or iterations == max_iterations:\n",
        "        print(\"---DECISION: FINISH---\")\n",
        "        return \"end\"\n",
        "    else:\n",
        "        print(\"---DECISION: RE-TRY SOLUTION---\")\n",
        "        return \"generate\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "f66b4e00-4731-42c8-bc38-72dd0ff7c92c",
      "metadata": {
        "id": "f66b4e00-4731-42c8-bc38-72dd0ff7c92c"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"generate\", generate)  # generation solution\n",
        "workflow.add_node(\"check_code\", code_check)  # check code\n",
        "\n",
        "# Build graph\n",
        "workflow.set_entry_point(\"generate\")\n",
        "workflow.add_edge(\"generate\", \"check_code\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"check_code\",\n",
        "    decide_to_finish,\n",
        "    {\n",
        "        \"end\": END,\n",
        "        \"generate\": \"generate\",\n",
        "    },\n",
        ")\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "9bcaafe4-ddcf-4fab-8620-2d9b6c508f98",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bcaafe4-ddcf-4fab-8620-2d9b6c508f98",
        "outputId": "abb39233-0ba6-4c54-ed68-ce5626cdae01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "content='A large language model is a type of artificial intelligence system that is trained on vast amounts of text data to understand and generate human-like text. These models are able to process and generate natural language text in a way that is coherent and contextually relevant. Large language models have been used in various applications such as machine translation, text generation, question answering, and more.' response_metadata={'token_usage': {'completion_tokens': 73, 'prompt_tokens': 28, 'total_tokens': 101}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3b956da36b', 'finish_reason': 'stop', 'logprobs': None} id='run-0f0880ce-cd52-40eb-9406-3e3fa3ede286-0'\n",
            "---NO CODE TEST FAILURES---\n",
            "---DECISION: FINISH---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'error': 'no',\n",
              " 'messages': [('user',\n",
              "   'How can I use langchain ask question: what is large language model ?'),\n",
              "  ('assistant',\n",
              "   'Using the LangChain library, we can create a simple application to ask a large language model (LLM) a question like \\'what is a large language model?\\'. The code will include initializing the LLM of choice, creating a prompt template, and invoking the LLM with the question. The following code assumes that the necessary packages have been installed and that the environment variable for the API key has been set as per the documentation. \\n Imports: from langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate \\n Code: llm = ChatOpenAI()\\n\\nprompt = ChatPromptTemplate.from_messages([\\n    (\"system\", \"You are an AI trained to answer questions accurately.\"),\\n    (\"user\", \"{input}\")\\n])\\n\\nresponse = (prompt | llm).invoke({\"input\": \"what is a large language model?\"})\\nprint(response)')],\n",
              " 'generation': code(prefix=\"Using the LangChain library, we can create a simple application to ask a large language model (LLM) a question like 'what is a large language model?'. The code will include initializing the LLM of choice, creating a prompt template, and invoking the LLM with the question. The following code assumes that the necessary packages have been installed and that the environment variable for the API key has been set as per the documentation.\", imports='from langchain_openai import ChatOpenAI\\nfrom langchain_core.prompts import ChatPromptTemplate', code='llm = ChatOpenAI()\\n\\nprompt = ChatPromptTemplate.from_messages([\\n    (\"system\", \"You are an AI trained to answer questions accurately.\"),\\n    (\"user\", \"{input}\")\\n])\\n\\nresponse = (prompt | llm).invoke({\"input\": \"what is a large language model?\"})\\nprint(response)', description='Schema for code solutions to questions about LCEL.'),\n",
              " 'iterations': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "question = \"How can I use langchain ask question: what is large language model ?\"\n",
        "app.invoke({\"messages\":[(\"user\",question)],\"iterations\":0})"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}